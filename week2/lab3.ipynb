{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: RAG\n",
    "\n",
    "## We will build and evaluate a Question Answering Expert for a fictional company: InsureLLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEFORE WE BEGIN:\n",
    "\n",
    "Look at the knowledge-base - this is the company shared drive.\n",
    "\n",
    "### For those new to RAG:\n",
    "\n",
    "Does one of the Experts want to give an explanation?\n",
    "\n",
    "We will be figuring out ways to insert relevant background information in to the prompt..\n",
    "\n",
    "Today will be more intense - please ask me lots of questions and clarifications.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from chromadb import PersistentClient\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "from litellm import acompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4.1-nano\"\n",
    "db_name = \"preprocessed_db\"\n",
    "collection_name = \"docs\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data with semantic chunking and pre-processing\n",
    "\n",
    "Loading in the data and splitting it into chunks with the help of an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_path = Path(\"knowledge-base\")\n",
    "documents = []\n",
    "\n",
    "for folder in base_path.iterdir():\n",
    "    doc_type = folder.name\n",
    "    for file in folder.rglob(\"*.md\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            documents.append({\n",
    "                \"type\": doc_type,\n",
    "                \"source\": file.as_posix(),\n",
    "                \"text\": f.read()\n",
    "            })\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result(BaseModel):\n",
    "    page_content: str\n",
    "    metadata: dict\n",
    "\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    headline: str = Field(\n",
    "        description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\",\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        description=\"A few sentences summarizing the content of this chunk to answer common questions\"\n",
    "    )\n",
    "    original_text: str = Field(\n",
    "        description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\"\n",
    "    )\n",
    "\n",
    "    def as_result(self, document):\n",
    "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
    "        return Result(\n",
    "            page_content=self.headline + \"\\n\\n\" + self.summary + \"\\n\\n\" + self.original_text,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "\n",
    "\n",
    "class Chunks(BaseModel):\n",
    "    chunks: list[Chunk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(document):\n",
    "    how_many = (len(document[\"text\"]) // 800) + 1\n",
    "    return f\"\"\"\n",
    "You take a document and you split the document into overlapping chunks for a KnowledgeBase.\n",
    "\n",
    "The document is from the shared drive of a company called Insurellm.\n",
    "The document is of type: {document[\"type\"]}\n",
    "The document has been retrieved from: {document[\"source\"]}\n",
    "\n",
    "A chatbot will use these chunks to answer questions about the company.\n",
    "You should divide up the document as you see fit, being sure that the entire document is returned in the chunks - don't leave anything out.\n",
    "This document should probably be split into {how_many} chunks, but you can have more or less as appropriate.\n",
    "There should be overlap between the chunks as appropriate; typically about 25% overlap or about 50 words, so you have the same text in multiple chunks for best retrieval results.\n",
    "\n",
    "For each chunk, you should provide a headline, a summary, and the original text of the chunk.\n",
    "Together your chunks should represent the entire document with overlap.\n",
    "\n",
    "Here is the document:\n",
    "\n",
    "{document[\"text\"]}\n",
    "\n",
    "Repond with the chunks.\n",
    "\"\"\"\n",
    "\n",
    "def make_messages(document):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": make_prompt(document)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "async def process_document(document):\n",
    "    messages = make_messages(document)\n",
    "    response = await acompletion(model=MODEL, messages=messages, response_format=Chunks)\n",
    "    reply = response.choices[0].message.content\n",
    "    doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
    "    return [chunk.as_result(document) for chunk in doc_as_chunks]\n",
    "\n",
    "\n",
    "async def create_chunks(documents, batch_size=5):\n",
    "    chunks = []\n",
    "    for i in tqdm(range(0, len(documents), batch_size)):\n",
    "        batch = documents[i : i + batch_size]\n",
    "        tasks = [process_document(doc) for doc in batch]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for result in results:\n",
    "            chunks.extend(result)\n",
    "    return chunks\n",
    "\n",
    "chunks = await create_chunks(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))\n",
    "chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks):\n",
    "    chroma = PersistentClient(path=db_name)\n",
    "    if collection_name in [c.name for c in chroma.list_collections()]:\n",
    "        chroma.delete_collection(collection_name)\n",
    "\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    emb = openai.embeddings.create(model=embedding_model, input=texts).data\n",
    "    vectors = [e.embedding for e in emb]\n",
    "\n",
    "    collection = chroma.get_or_create_collection(collection_name)\n",
    "\n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "    metas = [chunk.metadata for chunk in chunks]\n",
    "\n",
    "    collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metas)\n",
    "    print(f\"Vectorstore created with {collection.count()} documents\")\n",
    "\n",
    "create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = PersistentClient(path=db_name)\n",
    "collection = chroma.get_or_create_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many documents are in the vector store? How many dimensions?\n",
    "\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the vectors, documents and metadata\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['source'].split('/')[1] for metadata in metadatas]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = PersistentClient(path=db_name)\n",
    "collection = chroma.get_or_create_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(question, k=5):\n",
    "    query = openai.embeddings.create(model=embedding_model, input=[question]).data[0].embedding\n",
    "    results = collection.query(query_embeddings=[query], n_results=k)\n",
    "    chunks = []\n",
    "    for result in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        chunks.append(Result(page_content=result[0], metadata=result[1]))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "fetch_context(\"Who is Avery?\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Code to Call OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context(chunks):\n",
    "    result = \"\"\n",
    "    for chunk in chunks:\n",
    "        result += f\"Extract from {chunk.metadata['source']}:\\n{chunk.page_content}\\n\\n\"\n",
    "    return result\n",
    "\n",
    "def make_rag_prompt(question,chunks):\n",
    "    context = make_context(chunks)\n",
    "    return f\"\"\"\n",
    "The user has asked the following question:\n",
    "\n",
    "{question}\n",
    "\n",
    "For context, here are extracts from the Knowledge Base that might be relevant:\n",
    "\n",
    "{context}\n",
    "\n",
    "With this context, please answer the question. Reply only with the answer for the user.\n",
    "\"\"\"\n",
    "\n",
    "def make_rag_messages(question, chunks):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about the company Insurellm based on the context provided. If you don't know the answer, say so.\"},\n",
    "        {\"role\": \"user\", \"content\": make_rag_prompt(question, chunks)}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Avery?\"\n",
    "chunks = fetch_context(question)\n",
    "make_rag_messages(\"Who is Avery?\", chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    chunks = fetch_context(question)\n",
    "    messages = make_rag_messages(question, chunks)\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages, temperature=0)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Who is Avery?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHALLENGE:\n",
    "\n",
    "You will be changing or replacing 2 modules:\n",
    "\n",
    "`ingest.py`\n",
    "\n",
    "`answer.py`\n",
    "\n",
    "They are VERY simple! Let's look at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now check out ingest.py\n",
    "\n",
    "Then run at the terminal:\n",
    "\n",
    "`uv run ingest.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run ingest2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now check out answer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from answer2 import fetch_context, answer_question\n",
    "\n",
    "fetch_context(\"Who is Avery?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, chunks = await answer_question(\"Who is Avery?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now check out app.py\n",
    "\n",
    "As long as you keep the same 2 functions in `answer.py`, this UI will keep working!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK - Now it's time to EVALUATE!\n",
    "\n",
    "### First check out tests.jsonl for all the questions\n",
    "\n",
    "And see how it's loaded in test.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import load_tests\n",
    "\n",
    "test_data = load_tests()\n",
    "\n",
    "print(len(test_data))\n",
    "print(test_data[0])\n",
    "print(test_data[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(test.category for test in test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now take a look at eval.py\n",
    "\n",
    "test_data[0] is a very hard question that it sometimes gets wrong  \n",
    "test_data[1] is an easy question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import evaluate_retrieval, evaluate_answer\n",
    "\n",
    "evaluate_retrieval(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_answer(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND FINALLY - all come together in a UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run evaluator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for your experiments\n",
    "\n",
    "### Quick wins\n",
    "\n",
    "- Experiment with the encoder\n",
    "- Experiment with chunking strategies\n",
    "\n",
    "### Big change ideas\n",
    "\n",
    "1. Pre-processing - use an LLM to rewrite (a) the chunks and/or (b) the questions / conversation history\n",
    "2. Hierarchical RAG - summarize at different levels and do RAG over summaries\n",
    "3. Tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 RAG Techniques\n",
    "\n",
    "1. **Chunking R&D:** experiment with chunking strategy to optimize for your commercial goal\n",
    "2. **Encoder R&D:** select the best Encoder model based on a test set\n",
    "3. **Improve Prompts:** general content, the current date, relevant context and history\n",
    "4. **Document pre-processing:** use an LLM to make the chunks and/or text for encoding\n",
    "5. **Query rewriting:** use an LLM to convert the userâ€™s question to a RAG query\n",
    "6. **Query expansion:** use an LLM to turn the question into multiple RAG queries\n",
    "7. **Re-ranking:** use an LLM to sub-select from RAG results\n",
    "8. **Hierarchical:** use an LLM to summarize at multiple levels\n",
    "9. **Graph RAG:** retrieve content closely related to similar documents\n",
    "10. **Agentic RAG:** use Agents for retrieval, combining with Memory and Tools such as SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 hard questions that can be addressed with the above:\n",
    "\n",
    "- Who won the IIOTY award in 2023?\n",
    "\n",
    "- What proportion of employees have a salary over $90,000?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
