{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cd6350",
   "metadata": {},
   "source": [
    "## Lab 8: Inference and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261e41d",
   "metadata": {},
   "source": [
    "#### <span style=\"color: orange;\">Question: what is temperature and how does it work?</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8509ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"In one sentence, describe the color blue to someone who has never been able to see\"\n",
    "messages = [{\"role\": \"user\", \"content\": message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28727011",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages, temperature=0.0)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages, temperature=0.0)\n",
    "    display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages, temperature=1.85)\n",
    "    display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ff237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "import math\n",
    "\n",
    "class TokenPredictor:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.client = OpenAI()\n",
    "        self.messages = []\n",
    "        self.predictions = []\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def predict_tokens(self, prompt: str, max_tokens: int = 100) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate text token by token and track prediction probabilities.\n",
    "        Returns list of predictions with top token and alternatives.\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0,  # Use temperature 0 for deterministic output\n",
    "            logprobs=True,\n",
    "            seed=42,\n",
    "            top_logprobs=3,  # Get top 3 token predictions\n",
    "            stream=True  # Stream the response\n",
    "        )\n",
    "        \n",
    "        predictions = []\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                token = chunk.choices[0].delta.content\n",
    "                logprobs = chunk.choices[0].logprobs.content[0].top_logprobs\n",
    "                logprob_dict = {item.token: item.logprob for item in logprobs}\n",
    "                \n",
    "                # Get top predicted token and probability\n",
    "                top_token = token\n",
    "                top_prob = logprob_dict[token]\n",
    "                \n",
    "                # Get alternative predictions\n",
    "                alternatives = []\n",
    "                for alt_token, alt_prob in logprob_dict.items():\n",
    "                    if alt_token != token:\n",
    "                        alternatives.append((alt_token, math.exp(alt_prob)))\n",
    "                alternatives.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                prediction = {\n",
    "                    'token': top_token,\n",
    "                    'probability': math.exp(top_prob),\n",
    "                    'alternatives': alternatives[:2]  # Keep top 2 alternatives\n",
    "                }\n",
    "                predictions.append(prediction)\n",
    "                \n",
    "        return predictions\n",
    "\n",
    "def create_token_graph(model_name:str, predictions: List[Dict]) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create a directed graph showing token predictions and alternatives.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    G.add_node(\"START\", \n",
    "               token=model_name,\n",
    "               prob=\"START\",\n",
    "               color='lightgreen',\n",
    "               size=4000)\n",
    "    \n",
    "    # First, create all main token nodes in sequence\n",
    "    for i, pred in enumerate(predictions):\n",
    "        token_id = f\"t{i}\"\n",
    "        G.add_node(token_id, \n",
    "                  token=pred['token'],\n",
    "                  prob=f\"{pred['probability']*100:.1f}%\",\n",
    "                  color='lightblue',\n",
    "                  size=6000)\n",
    "        \n",
    "        if i == 0:\n",
    "            G.add_edge(\"START\", token_id)\n",
    "        else:\n",
    "            G.add_edge(f\"t{i-1}\", token_id)\n",
    "    \n",
    "    # Then add alternative nodes with a different y-position\n",
    "    last_id = None\n",
    "    for i, pred in enumerate(predictions):\n",
    "        parent_token = \"START\" if i == 0 else f\"t{i-1}\"\n",
    "        \n",
    "        # Add alternative token nodes slightly below main sequence\n",
    "        for j, (alt_token, alt_prob) in enumerate(pred['alternatives']):\n",
    "            alt_id = f\"t{i}_alt{j}\"\n",
    "            G.add_node(alt_id,\n",
    "                      token=alt_token,\n",
    "                      prob=f\"{alt_prob*100:.1f}%\",\n",
    "                      color='lightgray',\n",
    "                      size=6000)\n",
    "            \n",
    "            # Add edge from main token to its alternatives only\n",
    "            G.add_edge(parent_token, alt_id)\n",
    "            last_id = parent_token\n",
    "            \n",
    "\n",
    "    G.add_node(\"END\", \n",
    "               token=\"END\",\n",
    "               prob=\"100%\",\n",
    "               color='red',\n",
    "               size=6000)\n",
    "    G.add_edge(last_id, \"END\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "def visualize_predictions(G: nx.DiGraph, figsize=(14, 150)):\n",
    "    \"\"\"\n",
    "    Visualize the token prediction graph with vertical layout and alternating alternatives.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create custom positioning for nodes\n",
    "    pos = {}\n",
    "    spacing_y = 5  # Vertical spacing between main tokens\n",
    "    spacing_x = 5  # Horizontal spacing for alternatives\n",
    "    \n",
    "    # Position main token nodes in a vertical line\n",
    "    main_nodes = [n for n in G.nodes() if '_alt' not in n]\n",
    "    for i, node in enumerate(main_nodes):\n",
    "        pos[node] = (0, -i * spacing_y)  # Center main tokens vertically\n",
    "    \n",
    "    # Position alternative nodes to left and right of main tokens\n",
    "    for node in G.nodes():\n",
    "        if '_alt' in node:\n",
    "            main_token = node.split('_')[0]\n",
    "            alt_num = int(node.split('_alt')[1])\n",
    "            if main_token in pos:\n",
    "                # Place first alternative to left, second to right\n",
    "                x_offset = -spacing_x if alt_num == 0 else spacing_x\n",
    "                pos[node] = (x_offset, pos[main_token][1] + 0.05)\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_colors = [G.nodes[node]['color'] for node in G.nodes()]\n",
    "    node_sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes)\n",
    "\n",
    "    # Draw all edges as straight lines\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, arrowsize=20, alpha=0.7)\n",
    "    \n",
    "    # Add labels with token and probability\n",
    "    labels = {node: f\"{G.nodes[node]['token']}\\n{G.nodes[node]['prob']}\"\n",
    "              for node in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=14)\n",
    "    \n",
    "    plt.title(\"Token prediction.\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Adjust plot limits to ensure all nodes are visible\n",
    "    margin = 8\n",
    "    x_values = [x for x, y in pos.values()]\n",
    "    y_values = [y for x, y in pos.values()]\n",
    "    plt.xlim(min(x_values) - margin, max(x_values) + margin)\n",
    "    plt.ylim(min(y_values) - margin, max(y_values) + margin)\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4.1-mini\"\n",
    "\n",
    "predictor = TokenPredictor(model_name)\n",
    "\n",
    "predictions = predictor.predict_tokens(message)\n",
    "\n",
    "G = create_token_graph(model_name, predictions)\n",
    "visualize_predictions(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f1e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
